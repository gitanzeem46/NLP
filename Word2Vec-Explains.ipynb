{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a76ec165-c096-41ae-9d07-6714f77e5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d39341e1-f114-4996-bdc8-39f681298522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora # Yeh line Gensim library ke 'corpora' module ko import karti hai, jo document-term matrices banane aur manage karne ke kaam aata hai NLP mein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7640c6c1-e579-43e0-8b09-d2e626af547e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 0, 'world': 1, 'gensim': 2}\n"
     ]
    }
   ],
   "source": [
    "documents = [[\"hello\", \"world\"], [\"hello\", \"gensim\"]]\n",
    "dictionary = corpora.Dictionary(documents)  #Yeh line documents ka ek dictionary (vocabulary) banati hai, jismein har unique word ko ek unique ID di jaati hai.\n",
    "\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8afb1e85-57c6-4c46-9f7e-efa4341452c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "149f49c9-bddc-4bb4-9775-144dca9f2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example corpora \n",
    "sentences = [[\"this\",\"is\",\"a\",\"sample\"],[\"we\",\"is\",\"learning\",\"gensim\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a89c0511-139c-411e-851f-e02a5ca6dbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
      " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
      " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
      " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
      "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
      "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
      "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
      " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
      "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
      "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
      " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
      " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
      "  9.9641159e-03  1.8466286e-02]\n"
     ]
    }
   ],
   "source": [
    "# Train Word2vec model\n",
    "model = Word2Vec(sentences,vector_size=50,window=3,min_count=2, workers=4)\n",
    "print(model.wv[\"is\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f89d4bcb-a6b1-4c5c-a2e6-f2143acdb296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00178742  0.0007881   0.01701117  0.03003091 -0.03100983 -0.0237227\n",
      "  0.02152958  0.02990996 -0.01671809 -0.01254457  0.02460168 -0.00511157\n",
      " -0.01512204  0.02184684 -0.01620053 -0.00605339  0.0095886   0.00330625\n",
      " -0.02761738 -0.03149606  0.02437255  0.01690087  0.02252564  0.00254289\n",
      "  0.02116963 -0.01135122 -0.00315467  0.01922858 -0.02507213 -0.01312034]\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences,vector_size=30 , window=5, min_count=1,workers=4)\n",
    "print(model.wv[\"is\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6687574d-6001-4cef-a06c-06c0d0afa761",
   "metadata": {},
   "source": [
    "sentences â†’ Training ke liye input sentences (list of tokenized words).\n",
    "vector_size=50 â†’ Har word ka 50-dimensional vector banega.\n",
    "window=3 â†’ Ek word ke aage aur peeche 3 words tak ka context consider hoga.\n",
    "min_count=2 â†’ Sirf wahi words include honge jo kam se kam 2 baar aaye hain.\n",
    "workers=4 â†’ Model 4 CPU cores ka use karega training ke liye (multi-threading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a314a8e7-e0c4-4f18-a9a8-e911952f9109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess #  jo raw text ko clean aur tokenize karne ke liye use hota hai."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bb9e0fc-ff7b-4e1c-a912-1b6e4649f2ab",
   "metadata": {},
   "source": [
    "ðŸ“Œ simple_preprocess text ko clean aur tokenize karta hai NLP tasks ke liye.\n",
    "ðŸ“Œ Lowercasing, punctuation remove, tokenization, short words filtering karta hai.\n",
    "ðŸ“Œ NLP applications, Word2Vec aur deep learning models me preprocessing ke liye kaam aata hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "18ae7e9f-307c-4877-a22b-0ba1974be8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'gensim', 'is', 'great', 'for', 'topic', 'modeling']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello , world! Gensim is great for topic modeling.\"\n",
    "tokens = simple_preprocess(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0d766e67-b50c-482e-811a-469567e56fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'tanzeem', 'gensim', 'is', 'great', 'for', 'topic', 'modeling']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello , TANZEEM! GEnsim Is Great FOR topic Modeling.\"\n",
    "tokens = simple_preprocess(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e65077ae-cc81-4409-a933-c426a50712e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'tanzeem', 'gensim', 'is', 'great', 'for', 'topic', 'modeling']\n",
      "['hello', 'tanzeem', 'gensim', 'is', 'great', 'for', 'topic', 'modeling']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello , TANZEEM! GEnsim Is Great FOR topic Modeling.\"\n",
    "tokens = simple_preprocess(text)\n",
    "print(tokens)\n",
    "tokens = simple_preprocess(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b17d7020-037a-4ae2-b892-964c920dc959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize  #sent_tokenize ek function hai jo ek given text ko multiple sentences mein todne ka kaam karta hai.\n",
    "from gensim.utils import simple_preprocess #simple_preprocess function ko use karke aap raw text ko clean aur tokenized form mein convert kar sakte hain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1bc13e20-59f6-43f2-af94-84a78117bfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Hello This is Gensim and NLTK working together.', \"Let's preprocess text.\"]\n",
      "Processed Sentences: [['hello', 'this', 'is', 'gensim', 'and', 'nltk', 'working', 'together'], ['let', 'preprocess', 'text']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "\n",
    "text = \"Hello This is Gensim and NLTK working together. Let's preprocess text.\"\n",
    "\n",
    "# Split into sentences \n",
    "sentences = sent_tokenize(text)   \n",
    "print(\"Sentences:\", sentences)\n",
    "\n",
    "# Preprocess each sentence \n",
    "processed_sentences = [simple_preprocess(sentence) for sentence in sentences]\n",
    "\n",
    "print(\"Processed Sentences:\", processed_sentences)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a332488-c19e-48d8-927c-e65fc140a65b",
   "metadata": {},
   "source": [
    "âœ… \"Sentences:\" ek string hai.\n",
    "âœ… sentences ek list hai jisme sentences store hain.\n",
    "âœ… Comma , ka use karke hum dono values ko ek saath print kar sakte hain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6fd27735-1a82-424e-b518-f6eb1b8139f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world']\n",
      "['this', 'is', 'test']\n",
      "['gensim', 'is', 'great', 'for', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "sentences = [\"Hello world!\", \"This is a test.\", \"Gensim is great for NLP.\"]\n",
    "\n",
    "# Har sentence ko preprocess karke print karega\n",
    "for sentence in sentences:\n",
    "    processed = simple_preprocess(sentence)\n",
    "    print(processed)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c1b06f9-7731-43b5-b0ea-2139e63adb5d",
   "metadata": {},
   "source": [
    "Split into sentences:\n",
    "sent_tokenize(text) splits the input text into individual sentences.\n",
    "Preprocess each sentence:\n",
    "The list comprehension [simple_preprocess(sentence) for sentence in sentences] applies simple_preprocess to each sentence to clean and tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c893f36a-b583-4840-a13d-3ac14cbd1e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01631583  0.0089916  -0.00827415  0.00164907  0.01699724 -0.00892435\n",
      "  0.009035   -0.01357392 -0.00709698  0.01879702 -0.00315531  0.00064274\n",
      " -0.00828126 -0.01536538 -0.00301602  0.00493959 -0.00177605  0.01106732\n",
      " -0.00548595  0.00452013  0.01091159  0.01669191 -0.00290748 -0.01841629\n",
      "  0.0087411   0.00114357  0.01488382 -0.00162657 -0.00527683 -0.01750602\n",
      " -0.00171311  0.00565313  0.01080286  0.01410531 -0.01140624  0.00371764\n",
      "  0.01217773 -0.0095961  -0.00621452  0.01359526  0.00326295  0.00037983\n",
      "  0.00694727  0.00043555  0.01923765  0.01012121 -0.01783478 -0.01408312\n",
      "  0.00180291  0.01278507]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Example tokenized corpus (list of sentences, each sentence is a list of words)\n",
    "corpus = [\n",
    "    [\"word2vec\", \"is\", \"a\", \"powerful\", \"tool\", \"for\", \"nlp\"],\n",
    "    [\"gensim\", \"makes\", \"word2vec\", \"easy\", \"to\", \"use\"],\n",
    "    [\"deep\", \"learning\", \"models\", \"are\", \"great\", \"for\", \"language\", \"tasks\"],\n",
    "]\n",
    "\n",
    "# Initialize the Word2Vec model\n",
    "model1 = Word2Vec(\n",
    "    sentences=corpus,  # Training corpus\n",
    "    vector_size=50,     # Dimension of the word vectors\n",
    "    window=10,          # Context window size (how many words before and after the target word to consider)\n",
    "    min_count=2,        # Ignore words that appear less than twice\n",
    "    workers=4           # Number of CPU cores to use for training (multithreading)\n",
    ")\n",
    "\n",
    "# Example of printing the word vector for the word \"word2vec\"\n",
    "print(model1.wv[\"word2vec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23dd7113-c2d1-47c3-b4df-d0af2fbd401c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
      " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
      " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
      " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
      "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
      "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
      "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
      " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
      "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
      "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
      " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
      " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
      "  9.9641159e-03  1.8466286e-02]\n"
     ]
    }
   ],
   "source": [
    "# Acess the vector for a specific word \n",
    "vector = model.wv[\"is\"]\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "850802c9-52e4-4ba7-9c1b-5aee73bc951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "638c3013-22f0-456b-b014-16024521eae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nlp': 0}\n"
     ]
    }
   ],
   "source": [
    "#  sample tokenized corpus\n",
    "story = [\n",
    "    ['gensim','is','great','for','nlp'],\n",
    "    ['word2vec','creates','word','embeddings'],\n",
    "    ['nlp','tools','are','powerful'],\n",
    "]\n",
    "\n",
    "#Intialize the word2vec model\n",
    "model= Word2Vec(window=5,min_count=2)\n",
    "\n",
    "# Build the vacabulary from the corpus\n",
    "model.build_vocab(story)                 # âœ… build_vocab() function corpus se vocabulary create karta hai.\n",
    "\n",
    "                                        # Vocabulary ka matlab hai ki corpus mein jitne unique words hain, unka ek list (set) banata hai.\n",
    "\n",
    "#check the vacabulary\n",
    "print(model.wv.key_to_index)      # model.wv.key_to_index ek dictionary hai jo word-to-index mapping dikhata hai.\n",
    "\n",
    "                                 # key_to_index dictionary har word ko ek unique index assign karta hai, jo us word ki position ko show karta hai.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b0256-aa35-4741-9163-f6d97492edf2",
   "metadata": {},
   "source": [
    "## what is a build vocab?\n",
    "\n",
    "it identifies all uniques words in the corpus and determine whether they need\n",
    "the criteria set by the parametes\n",
    "\n",
    "like minimum count. This mothod scans the corpus and creates a dictionary of all\n",
    "unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dc50a7a-92fe-494e-a3a3-fd97960394bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('learning', 0.15016481280326843), ('makes', 0.12813477218151093), ('is', 0.0931011214852333), ('a', 0.09215974062681198), ('for', 0.046526193618774414), ('use', 0.000730697123799473), ('nlp', -0.003644448472186923), ('word2vec', -0.003701347392052412), ('great', -0.009253417141735554), ('gensim', -0.030302342027425766)]\n"
     ]
    }
   ],
   "source": [
    "# sample tokenized corpus\n",
    "story =[\n",
    "    ['word2vec','is','a','powerful','tool'],\n",
    "    ['gensim','makes','it','easy','to','use'],\n",
    "    ['deep','learning','is','great','for','nlp']\n",
    "]\n",
    "\n",
    "# Inltialize the words2vec model\n",
    "model=Word2Vec(window=5,min_count=1)\n",
    "\n",
    "# Build the vocabulary\n",
    "model.build_vocab(story)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.train(story,total_examples=model.corpus_count,epochs=model.epochs)\n",
    "\n",
    "# Access word vectors\n",
    "vector = model.wv['word2vec'] # vector for the word 'word2vec'\n",
    "\n",
    "\n",
    "# find similar words\n",
    "similar_words=model.wv.most_similar('tool')\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cbe34a2-1640-4d03-8128-604a07e4becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# its specifies The total number of sentences in training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb33ad-ecea-435a-988d-cb997697a1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2ef47-fc80-407b-ae14-3bb8f8ddcb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece24cc-a61c-4e61-b8b2-4a363bfe503a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
