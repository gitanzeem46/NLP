{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ec62e7-cb45-428d-85d8-df46cd4fb959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c4f543c-7928-4a73-b17a-94e343b06d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai' 'learning' 'machine']\n",
      "[[0 1 1]\n",
      " [0 2 1]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\"Machine learning is amazing\",\n",
    "             \"Deep learning is a subset of machine learning.\",\n",
    "             \"Natural language processing is a part of AI.\",\n",
    "             \"AI is transforming the world.\"]\n",
    "\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words=\"english\")\n",
    "dtm = cv.fit_transform(documents)  # Convert text to document-term matrix\n",
    "\n",
    "print(cv.get_feature_names_out())  # Display remaining words\n",
    "print(dtm.toarray())  # Show word counts in documents"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34b48416-8144-4c28-880a-95335ef66a7e",
   "metadata": {},
   "source": [
    "max_df=0.95 → Jo words 95% documents me hai, unko ignore karega (kyunki woh common words ho sakte hain).\n",
    "min_df=2 → Jo words kam se kam 2 documents me hai sirf unko consider karega.\n",
    "stop_words=\"english\" → English ke common stop words (e.g., \"is\", \"the\", \"a\") ko remove karega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b95a8de7-23fa-48a2-ac88-06e95277409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b238a08e-2d15-46f5-8b70-420e411ab79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "subset\n",
      "amazing\n",
      "learning\n",
      "deep\n",
      "ai\n",
      "natural\n",
      "processing\n",
      "leraning\n",
      "subset\n"
     ]
    }
   ],
   "source": [
    "# Sample text corpus \n",
    "documents = [\n",
    "    \"Machine leraning is amazing.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Natural language processing is a part of AI.\",\n",
    "    \"AI is transforming the world.\"\n",
    "]\n",
    "\n",
    "\n",
    "#Convert text into a doucments-term matrix\n",
    "cv = CountVectorizer(stop_words=\"english\")\n",
    "dtm = cv.fit_transform(documents) \n",
    "\n",
    "# Get features names \n",
    "feature_names = cv.get_feature_names_out() # updated method\n",
    "\n",
    "\n",
    "# Generate 10 random words from the vocuabulary\n",
    "for i in range(10):\n",
    "    random_word_id = random.randint(0,len(feature_names)-1)  # use actual vacuabulary size\n",
    "    print(feature_names[random_word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2957ef70-feec-488b-835a-e43116f6f25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: Topic 1\n",
      "Document 2: Topic 0\n",
      "Document 3: Topic 1\n",
      "Document 4: Topic 0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Sample corpus\n",
    "documents = [\n",
    "    \"I love machine learning and data science\",\n",
    "    \"Deep learning is a subset of machine learning\",\n",
    "    \"I enjoy playing football and watching sports\",\n",
    "    \"Sports analytics is an interesting field\",\n",
    "]\n",
    "\n",
    "# Convert text to document-term matrix\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "dtm = vectorizer.fit_transform(documents)  # FIXED: Fit and transform the text\n",
    "\n",
    "# Train LDA model\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Get topic distribution for each document\n",
    "topic_results = lda.transform(dtm)  # shape: (num_docs, num_topics)\n",
    "\n",
    "# Identify the dominant topic for each document\n",
    "dominant_topics = topic_results.argmax(axis=1)  # FIXED: Correct variable name\n",
    "\n",
    "# Print results\n",
    "for i, topic in enumerate(dominant_topics):\n",
    "    print(f\"Document {i+1}: Topic {topic}\")  # Fixed formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd2f635-db12-4091-b8ff-e5af1ad0262e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
